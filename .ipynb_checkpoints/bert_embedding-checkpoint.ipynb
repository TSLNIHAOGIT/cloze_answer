{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[\n",
    "    {\n",
    "        \"paragraphs\":{\n",
    "        \"id\":\"TRAIN_0\",\n",
    "        \"company_name\":'重庆市富泰通物流有限公司',\n",
    "        'context':[\t\t\t['中华人民共和国海关出口货物报关单', 1207, 108, 71, 1109],\t\t\t['仅供内部核对', 282, 129, 88, 534],\t\t\t['页码/页数: 1/3', 3140, 283, 38, 263],\t\t\t['预录入编号:', 128, 295, 38, 213],\t\t\t['海关编号:', 1003, 295, 38, 179],\t\t\t['FSF-9A0009', 340, 295, 46, 229],\t\t\t['(西永综保)', 1678, 295, 46, 221],\t\t\t['境内发货人', 128, 362, 38, 196],\t\t\t['出境关别', 1003, 362, 38, 163],\t\t\t['出口日期', 1536, 362, 38, 163],\t\t\t['申报日期', 2186, 362, 38, 163],\t\t\t['备案号', 2761, 362, 38, 129],\t\t\t['(5006660007/91500106691231173P)', 378, 362, 34, 492],\t\t\t['(7910)', 1190, 362, 34, 109],\t\t\t['重庆市富泰通物流有限公司', 128, 404, 46, 525],\t\t\t['蓉机快件', 1003, 404, 46, 192],\t\t\t['20191011', 1536, 404, 46, 184],\t\t\t['20191011', 2182, 404, 46, 184],\t\t\t['H80136000013', 2761, 404, 46, 271],\t\t\t['境外收货人', 128, 454, 38, 196],\t\t\t['运输方式', 1003, 454, 38, 163],\t\t\t['运输工具名称及航次号', 1536, 454, 38, 367],\t\t\t['提运单号', 2186, 454, 38, 163],\t\t\t['(5)', 1190, 454, 34, 63],\t\t\t['航空运输', 1003, 495, 46, 192],\t\t\t['Broadcom Limited', 136, 495, 34, 263],\t\t\t['生产销售单位', 128, 549, 38, 229],\t\t\t['监管方式', 1003, 549, 38, 163],\t\t\t['征免性质', 1536, 549, 38, 163],\t\t\t['许可证号', 2186, 549, 38, 163],\t\t\t['(5006660007/91500106691231173P)', 378, 549, 34, 492],\t\t\t['(5034)', 1190, 549, 34, 109],\t\t\t['(000)', 1736, 549, 34, 96],\t\t\t['重庆市富泰通物流有限公司', 128, 591, 46, 525],\t\t\t['区内物流货物', 1003, 591, 46, 275],\t\t\t['性质为空', 1536, 591, 46, 192],\t\t\t['贸易国（地区）', 1003, 645, 38, 263],\t\t\t['运抵国（地区）', 1536, 645, 38, 263],\t\t\t['指运港', 2186, 645, 38, 129],\t\t\t['离境口岸', 2761, 645, 38, 163],\t\t\t['合同协议号', 128, 645, 38, 196],\t\t\t['(USA)', 1299, 645, 34, 109],\t\t\t['(USA)', 1828, 645, 34, 109],\t\t\t['(USA000)', 2373, 645, 34, 159],\t\t\t['(510005)', 2953, 645, 34, 142],\t\t\t['美国', 1003, 687, 46, 109],\t\t\t['美国', 1536, 687, 46, 109],\t\t\t['美国', 2186, 687, 46, 109],\t\t\t['成都双流国际机场国际快件', 2761, 687, 46, 525],\t\t\t['包装种类', 128, 737, 38, 163],\t\t\t['件数', 1003, 737, 38, 96],\t\t\t['毛重（千克）', 1157, 737, 38, 229],\t\t\t['净重（千克）', 1536, 737, 38, 229],\t\t\t['成交方式(7)', 1923, 737, 38, 204],\t\t\t['运费', 2186, 737, 38, 96],\t\t\t['保费', 2586, 737, 38, 96],\t\t\t['杂费', 2986, 737, 38, 96],\t\t\t['(92/22)', 378, 737, 34, 117],\t\t\t['再生木托/纸制或纤维板制盒/箱', 128, 779, 46, 588],\t\t\t['1', 1003, 779, 46, 46],\t\t\t['127.5', 1157, 779, 46, 117],\t\t\t['68.52233', 1536, 779, 46, 175],\t\t\t['EXW', 1923, 779, 46, 117],\t\t\t['随附单证及编号', 128, 833, 38, 263],\t\t\t['标记唛码及备注', 128, 929, 38, 263],\t\t\t['标记唛码:N/M', 436, 929, 46, 279],\t\t\t['备注:一体化/自行运输 /先出后报/DHL快件 分送单号:CFT-9A001C', 128, 979, 38, 1075],\t\t\t['项号', 140, 1104, 38, 96],\t\t\t['商品编码', 269, 1104, 38, 163],\t\t\t['商品名称及规格型号', 828, 1104, 38, 334],\t\t\t['数量及单位', 1753, 1104, 38, 196],\t\t\t['单价/总价/币制', 1944, 1104, 38, 288],\t\t\t['原产国（地区）', 2240, 1104, 38, 263],\t\t\t['最终目的国（地区）', 2503, 1104, 38, 334],\t\t\t['境内货源地', 2965, 1104, 38, 196],\t\t\t['征免', 3286, 1104, 38, 96],\t\t\t['1', 128, 1158, 34, 42],\t\t\t['8536901100', 253, 1158, 34, 184],\t\t\t['36伏及以内的接插连接件', 428, 1158, 34, 392],\t\t\t['1.584千克', 1786, 1158, 34, 163],\t\t\t['2.14', 2115, 1158, 34, 84],\t\t\t['中国', 2398, 1158, 34, 96],\t\t\t['美国', 2707, 1158, 34, 96],\t\t\t['(50066/500106)重庆西永综合', 2832, 1158, 34, 429],\t\t\t['全免', 3286, 1158, 34, 96]],\n",
    "        'qas':[\n",
    "            {\"question\":['境内发货人', 128, 362, 38, 196],\n",
    "            'id':'TRAIN_0_QUERY_0',\n",
    "            \"answers\": ['重庆市富泰通物流有限公司', 128, 591, 46, 525],\n",
    "            },\n",
    "             {\"question\":['境外收货人', 128, 454, 38, 196],\n",
    "            'id':'TRAIN_0_QUERY_1',\n",
    "            \"answers\": ['Broadcom Limited', 136, 495, 34, 263],\n",
    "            },\n",
    "             {\"question\":['贸易国（地区）', 1003, 645, 38, 263],\n",
    "            'id':'TRAIN_0_QUERY_2',\n",
    "            \"answers\": ['美国', 1003, 687, 46, 109],\n",
    "            },\n",
    "             {\"question\":['运抵国（地区）', 1536, 645, 38, 263],\n",
    "            'id':'TRAIN_0_QUERY_3',\n",
    "            \"answers\": ['美国', 1536, 687, 46, 109],\n",
    "            },\n",
    "             {\"question\": ['指运港', 2186, 645, 38, 129],\n",
    "            'id':'TRAIN_0_QUERY_4',\n",
    "            \"answers\": ['美国', 2186, 687, 46, 109],\n",
    "            },\n",
    "            \n",
    "        ]\n",
    "        }\n",
    "    \n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'重庆市富泰通物流有限公司'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]['paragraphs']['company_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRAIN_0'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]['paragraphs']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answers': ['重庆市富泰通物流有限公司', 128, 591, 46, 525],\n",
       "  'id': 'TRAIN_0_QUERY_0',\n",
       "  'question': ['境内发货人', 128, 362, 38, 196]},\n",
       " {'answers': ['Broadcom Limited', 136, 495, 34, 263],\n",
       "  'id': 'TRAIN_0_QUERY_1',\n",
       "  'question': ['境外收货人', 128, 454, 38, 196]},\n",
       " {'answers': ['美国', 1003, 687, 46, 109],\n",
       "  'id': 'TRAIN_0_QUERY_2',\n",
       "  'question': ['贸易国（地区）', 1003, 645, 38, 263]},\n",
       " {'answers': ['美国', 1536, 687, 46, 109],\n",
       "  'id': 'TRAIN_0_QUERY_3',\n",
       "  'question': ['运抵国（地区）', 1536, 645, 38, 263]},\n",
       " {'answers': ['美国', 2186, 687, 46, 109],\n",
       "  'id': 'TRAIN_0_QUERY_4',\n",
       "  'question': ['指运港', 2186, 645, 38, 129]}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]['paragraphs']['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': {'company_name': '重庆市富泰通物流有限公司',\n",
       "  'context': [['中华人民共和国海关出口货物报关单', 1207, 108, 71, 1109],\n",
       "   ['仅供内部核对', 282, 129, 88, 534],\n",
       "   ['页码/页数: 1/3', 3140, 283, 38, 263],\n",
       "   ['预录入编号:', 128, 295, 38, 213],\n",
       "   ['海关编号:', 1003, 295, 38, 179],\n",
       "   ['FSF-9A0009', 340, 295, 46, 229],\n",
       "   ['(西永综保)', 1678, 295, 46, 221],\n",
       "   ['境内发货人', 128, 362, 38, 196],\n",
       "   ['出境关别', 1003, 362, 38, 163],\n",
       "   ['出口日期', 1536, 362, 38, 163],\n",
       "   ['申报日期', 2186, 362, 38, 163],\n",
       "   ['备案号', 2761, 362, 38, 129],\n",
       "   ['(5006660007/91500106691231173P)', 378, 362, 34, 492],\n",
       "   ['(7910)', 1190, 362, 34, 109],\n",
       "   ['重庆市富泰通物流有限公司', 128, 404, 46, 525],\n",
       "   ['蓉机快件', 1003, 404, 46, 192],\n",
       "   ['20191011', 1536, 404, 46, 184],\n",
       "   ['20191011', 2182, 404, 46, 184],\n",
       "   ['H80136000013', 2761, 404, 46, 271],\n",
       "   ['境外收货人', 128, 454, 38, 196],\n",
       "   ['运输方式', 1003, 454, 38, 163],\n",
       "   ['运输工具名称及航次号', 1536, 454, 38, 367],\n",
       "   ['提运单号', 2186, 454, 38, 163],\n",
       "   ['(5)', 1190, 454, 34, 63],\n",
       "   ['航空运输', 1003, 495, 46, 192],\n",
       "   ['Broadcom Limited', 136, 495, 34, 263],\n",
       "   ['生产销售单位', 128, 549, 38, 229],\n",
       "   ['监管方式', 1003, 549, 38, 163],\n",
       "   ['征免性质', 1536, 549, 38, 163],\n",
       "   ['许可证号', 2186, 549, 38, 163],\n",
       "   ['(5006660007/91500106691231173P)', 378, 549, 34, 492],\n",
       "   ['(5034)', 1190, 549, 34, 109],\n",
       "   ['(000)', 1736, 549, 34, 96],\n",
       "   ['重庆市富泰通物流有限公司', 128, 591, 46, 525],\n",
       "   ['区内物流货物', 1003, 591, 46, 275],\n",
       "   ['性质为空', 1536, 591, 46, 192],\n",
       "   ['贸易国（地区）', 1003, 645, 38, 263],\n",
       "   ['运抵国（地区）', 1536, 645, 38, 263],\n",
       "   ['指运港', 2186, 645, 38, 129],\n",
       "   ['离境口岸', 2761, 645, 38, 163],\n",
       "   ['合同协议号', 128, 645, 38, 196],\n",
       "   ['(USA)', 1299, 645, 34, 109],\n",
       "   ['(USA)', 1828, 645, 34, 109],\n",
       "   ['(USA000)', 2373, 645, 34, 159],\n",
       "   ['(510005)', 2953, 645, 34, 142],\n",
       "   ['美国', 1003, 687, 46, 109],\n",
       "   ['美国', 1536, 687, 46, 109],\n",
       "   ['美国', 2186, 687, 46, 109],\n",
       "   ['成都双流国际机场国际快件', 2761, 687, 46, 525],\n",
       "   ['包装种类', 128, 737, 38, 163],\n",
       "   ['件数', 1003, 737, 38, 96],\n",
       "   ['毛重（千克）', 1157, 737, 38, 229],\n",
       "   ['净重（千克）', 1536, 737, 38, 229],\n",
       "   ['成交方式(7)', 1923, 737, 38, 204],\n",
       "   ['运费', 2186, 737, 38, 96],\n",
       "   ['保费', 2586, 737, 38, 96],\n",
       "   ['杂费', 2986, 737, 38, 96],\n",
       "   ['(92/22)', 378, 737, 34, 117],\n",
       "   ['再生木托/纸制或纤维板制盒/箱', 128, 779, 46, 588],\n",
       "   ['1', 1003, 779, 46, 46],\n",
       "   ['127.5', 1157, 779, 46, 117],\n",
       "   ['68.52233', 1536, 779, 46, 175],\n",
       "   ['EXW', 1923, 779, 46, 117],\n",
       "   ['随附单证及编号', 128, 833, 38, 263],\n",
       "   ['标记唛码及备注', 128, 929, 38, 263],\n",
       "   ['标记唛码:N/M', 436, 929, 46, 279],\n",
       "   ['备注:一体化/自行运输 /先出后报/DHL快件 分送单号:CFT-9A001C', 128, 979, 38, 1075],\n",
       "   ['项号', 140, 1104, 38, 96],\n",
       "   ['商品编码', 269, 1104, 38, 163],\n",
       "   ['商品名称及规格型号', 828, 1104, 38, 334],\n",
       "   ['数量及单位', 1753, 1104, 38, 196],\n",
       "   ['单价/总价/币制', 1944, 1104, 38, 288],\n",
       "   ['原产国（地区）', 2240, 1104, 38, 263],\n",
       "   ['最终目的国（地区）', 2503, 1104, 38, 334],\n",
       "   ['境内货源地', 2965, 1104, 38, 196],\n",
       "   ['征免', 3286, 1104, 38, 96],\n",
       "   ['1', 128, 1158, 34, 42],\n",
       "   ['8536901100', 253, 1158, 34, 184],\n",
       "   ['36伏及以内的接插连接件', 428, 1158, 34, 392],\n",
       "   ['1.584千克', 1786, 1158, 34, 163],\n",
       "   ['2.14', 2115, 1158, 34, 84],\n",
       "   ['中国', 2398, 1158, 34, 96],\n",
       "   ['美国', 2707, 1158, 34, 96],\n",
       "   ['(50066/500106)重庆西永综合', 2832, 1158, 34, 429],\n",
       "   ['全免', 3286, 1158, 34, 96]],\n",
       "  'id': 'TRAIN_0',\n",
       "  'qas': [{'answers': ['重庆市富泰通物流有限公司', 128, 591, 46, 525],\n",
       "    'id': 'TRAIN_0_QUERY_0',\n",
       "    'question': ['境内发货人', 128, 362, 38, 196]},\n",
       "   {'answers': ['Broadcom Limited', 136, 495, 34, 263],\n",
       "    'id': 'TRAIN_0_QUERY_1',\n",
       "    'question': ['境外收货人', 128, 454, 38, 196]},\n",
       "   {'answers': ['美国', 1003, 687, 46, 109],\n",
       "    'id': 'TRAIN_0_QUERY_2',\n",
       "    'question': ['贸易国（地区）', 1003, 645, 38, 263]},\n",
       "   {'answers': ['美国', 1536, 687, 46, 109],\n",
       "    'id': 'TRAIN_0_QUERY_3',\n",
       "    'question': ['运抵国（地区）', 1536, 645, 38, 263]},\n",
       "   {'answers': ['美国', 2186, 687, 46, 109],\n",
       "    'id': 'TRAIN_0_QUERY_4',\n",
       "    'question': ['指运港', 2186, 645, 38, 129]}]}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               qas_id,\n",
    "               question_text,\n",
    "               doc_tokens,\n",
    "               orig_answer_text=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               is_impossible=False):\n",
    "    self.qas_id = qas_id\n",
    "    self.question_text = question_text\n",
    "    self.doc_tokens = doc_tokens\n",
    "    self.orig_answer_text = orig_answer_text\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.is_impossible = is_impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_data): \n",
    "    for entry in input_data:\n",
    "                paragraph_text = entry[\"paragraphs\"][\"context\"]\n",
    "               \n",
    "                print(paragraph_text)\n",
    "                for qa in entry[\"paragraphs\"][\"qas\"]:\n",
    "                         qas_id = qa[\"id\"]\n",
    "                         question_text = qa[\"question\"]\n",
    "                         answer = qa[\"answers\"]\n",
    "                         print(qas_id ,question_text,answer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['中华人民共和国海关出口货物报关单', 1207, 108, 71, 1109], ['仅供内部核对', 282, 129, 88, 534], ['页码/页数: 1/3', 3140, 283, 38, 263], ['预录入编号:', 128, 295, 38, 213], ['海关编号:', 1003, 295, 38, 179], ['FSF-9A0009', 340, 295, 46, 229], ['(西永综保)', 1678, 295, 46, 221], ['境内发货人', 128, 362, 38, 196], ['出境关别', 1003, 362, 38, 163], ['出口日期', 1536, 362, 38, 163], ['申报日期', 2186, 362, 38, 163], ['备案号', 2761, 362, 38, 129], ['(5006660007/91500106691231173P)', 378, 362, 34, 492], ['(7910)', 1190, 362, 34, 109], ['重庆市富泰通物流有限公司', 128, 404, 46, 525], ['蓉机快件', 1003, 404, 46, 192], ['20191011', 1536, 404, 46, 184], ['20191011', 2182, 404, 46, 184], ['H80136000013', 2761, 404, 46, 271], ['境外收货人', 128, 454, 38, 196], ['运输方式', 1003, 454, 38, 163], ['运输工具名称及航次号', 1536, 454, 38, 367], ['提运单号', 2186, 454, 38, 163], ['(5)', 1190, 454, 34, 63], ['航空运输', 1003, 495, 46, 192], ['Broadcom Limited', 136, 495, 34, 263], ['生产销售单位', 128, 549, 38, 229], ['监管方式', 1003, 549, 38, 163], ['征免性质', 1536, 549, 38, 163], ['许可证号', 2186, 549, 38, 163], ['(5006660007/91500106691231173P)', 378, 549, 34, 492], ['(5034)', 1190, 549, 34, 109], ['(000)', 1736, 549, 34, 96], ['重庆市富泰通物流有限公司', 128, 591, 46, 525], ['区内物流货物', 1003, 591, 46, 275], ['性质为空', 1536, 591, 46, 192], ['贸易国（地区）', 1003, 645, 38, 263], ['运抵国（地区）', 1536, 645, 38, 263], ['指运港', 2186, 645, 38, 129], ['离境口岸', 2761, 645, 38, 163], ['合同协议号', 128, 645, 38, 196], ['(USA)', 1299, 645, 34, 109], ['(USA)', 1828, 645, 34, 109], ['(USA000)', 2373, 645, 34, 159], ['(510005)', 2953, 645, 34, 142], ['美国', 1003, 687, 46, 109], ['美国', 1536, 687, 46, 109], ['美国', 2186, 687, 46, 109], ['成都双流国际机场国际快件', 2761, 687, 46, 525], ['包装种类', 128, 737, 38, 163], ['件数', 1003, 737, 38, 96], ['毛重（千克）', 1157, 737, 38, 229], ['净重（千克）', 1536, 737, 38, 229], ['成交方式(7)', 1923, 737, 38, 204], ['运费', 2186, 737, 38, 96], ['保费', 2586, 737, 38, 96], ['杂费', 2986, 737, 38, 96], ['(92/22)', 378, 737, 34, 117], ['再生木托/纸制或纤维板制盒/箱', 128, 779, 46, 588], ['1', 1003, 779, 46, 46], ['127.5', 1157, 779, 46, 117], ['68.52233', 1536, 779, 46, 175], ['EXW', 1923, 779, 46, 117], ['随附单证及编号', 128, 833, 38, 263], ['标记唛码及备注', 128, 929, 38, 263], ['标记唛码:N/M', 436, 929, 46, 279], ['备注:一体化/自行运输 /先出后报/DHL快件 分送单号:CFT-9A001C', 128, 979, 38, 1075], ['项号', 140, 1104, 38, 96], ['商品编码', 269, 1104, 38, 163], ['商品名称及规格型号', 828, 1104, 38, 334], ['数量及单位', 1753, 1104, 38, 196], ['单价/总价/币制', 1944, 1104, 38, 288], ['原产国（地区）', 2240, 1104, 38, 263], ['最终目的国（地区）', 2503, 1104, 38, 334], ['境内货源地', 2965, 1104, 38, 196], ['征免', 3286, 1104, 38, 96], ['1', 128, 1158, 34, 42], ['8536901100', 253, 1158, 34, 184], ['36伏及以内的接插连接件', 428, 1158, 34, 392], ['1.584千克', 1786, 1158, 34, 163], ['2.14', 2115, 1158, 34, 84], ['中国', 2398, 1158, 34, 96], ['美国', 2707, 1158, 34, 96], ['(50066/500106)重庆西永综合', 2832, 1158, 34, 429], ['全免', 3286, 1158, 34, 96]]\n",
      "TRAIN_0_QUERY_0 ['境内发货人', 128, 362, 38, 196] ['重庆市富泰通物流有限公司', 128, 591, 46, 525]\n",
      "TRAIN_0_QUERY_1 ['境外收货人', 128, 454, 38, 196] ['Broadcom Limited', 136, 495, 34, 263]\n",
      "TRAIN_0_QUERY_2 ['贸易国（地区）', 1003, 645, 38, 263] ['美国', 1003, 687, 46, 109]\n",
      "TRAIN_0_QUERY_3 ['运抵国（地区）', 1536, 645, 38, 263] ['美国', 1536, 687, 46, 109]\n",
      "TRAIN_0_QUERY_4 ['指运港', 2186, 645, 38, 129] ['美国', 2186, 687, 46, 109]\n"
     ]
    }
   ],
   "source": [
    "read_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling keras-bert-0.80.0:\r\n",
      "  Successfully uninstalled keras-bert-0.80.0\r\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y keras-bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir  keras-bert -i https://pypi.douban.com/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '范',\n",
       " '廷',\n",
       " '颂',\n",
       " '枢',\n",
       " '机',\n",
       " '（',\n",
       " '，',\n",
       " '）',\n",
       " '，',\n",
       " '圣',\n",
       " '名',\n",
       " '保',\n",
       " '禄',\n",
       " '·',\n",
       " '若',\n",
       " '瑟',\n",
       " '（',\n",
       " '）',\n",
       " '，',\n",
       " '是',\n",
       " '越',\n",
       " '南',\n",
       " '罗',\n",
       " '马',\n",
       " '天',\n",
       " '主',\n",
       " '教',\n",
       " '枢',\n",
       " '机',\n",
       " '。',\n",
       " '1963',\n",
       " '年',\n",
       " '10',\n",
       " '月',\n",
       " '1',\n",
       " '日',\n",
       " '被',\n",
       " '任',\n",
       " '为',\n",
       " '主',\n",
       " '教',\n",
       " '；',\n",
       " '1990',\n",
       " '年',\n",
       " '被',\n",
       " '擢',\n",
       " '升',\n",
       " '为',\n",
       " '天',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import codecs\n",
    "\n",
    "\n",
    "config_path = 'chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "# tokenizer = OurTokenizer(token_dict)\n",
    "tokenizer=Tokenizer(token_dict)\n",
    "##原始的token是正确的，数字年份这些都是分对的，被变换之后就分开了，显然不能如此粗糙的进行操作\n",
    "tokenizer.tokenize(u\"范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年10月1日被任为主教；1990年被擢升为天\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(extract_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.2892713 ,  0.17825669,  0.2993785 , ..., -0.2113975 ,\n",
       "          0.59805787, -0.29044253],\n",
       "        [-0.3438177 ,  0.320658  ,  0.8245277 , ..., -1.138897  ,\n",
       "         -0.29988754, -0.21084712],\n",
       "        [ 0.13761272, -0.26554742, -0.3197287 , ...,  0.27571467,\n",
       "          0.88094664, -0.03064159],\n",
       "        ...,\n",
       "        [ 0.5913439 , -0.40090016,  0.26507577, ...,  0.01675153,\n",
       "          0.7664653 ,  0.15207137],\n",
       "        [ 1.0636169 ,  0.2049691 , -0.54199594, ..., -0.11642104,\n",
       "          0.6464775 , -0.0324977 ],\n",
       "        [ 0.18561669,  0.515595  ,  0.6400809 , ..., -0.48827365,\n",
       "          0.6927049 , -0.1900103 ]], dtype=float32),\n",
       " array([[ 0.11605157,  0.2568603 ,  0.38559213, ..., -0.31201068,\n",
       "          0.747278  , -0.76422054],\n",
       "        [ 0.47432435, -0.12615255,  0.64729345, ..., -1.2681056 ,\n",
       "          0.18395618, -0.23168242],\n",
       "        [-0.31203026, -0.9601677 , -0.5388694 , ..., -0.5950943 ,\n",
       "          1.4349763 , -0.5048409 ],\n",
       "        ...,\n",
       "        [ 1.0442544 ,  0.24622692, -0.9987614 , ...,  0.84886587,\n",
       "          1.5352253 , -1.0861855 ],\n",
       "        [ 0.7917445 ,  0.5625421 , -0.87522304, ...,  0.12993982,\n",
       "          0.925221  , -0.43958238],\n",
       "        [ 0.3991019 ,  0.7471862 ,  0.2522347 , ..., -0.2801164 ,\n",
       "          0.5351868 , -0.8457844 ]], dtype=float32),\n",
       " array([[ 0.4833035 ,  0.56289166, -0.1767461 , ..., -0.00490585,\n",
       "          0.21913683, -0.3240456 ],\n",
       "        [ 0.5419348 ,  0.4947289 ,  1.1736306 , ..., -0.76104987,\n",
       "         -0.70704406, -0.15181598],\n",
       "        [ 0.6207222 ,  0.04165254, -0.12438626, ...,  0.32744983,\n",
       "          0.3059364 , -0.00872442],\n",
       "        ...,\n",
       "        [ 0.2646523 ,  0.06566699,  0.5619201 , ...,  1.0430295 ,\n",
       "          1.0301532 , -0.729521  ],\n",
       "        [ 0.6580208 ,  0.6137726 ,  0.44132534, ..., -0.25967285,\n",
       "          1.1366646 , -0.46149558],\n",
       "        [ 0.63622683,  0.04755419,  0.30563667, ...,  0.28557417,\n",
       "          0.15210378, -0.54650956]], dtype=float32),\n",
       " array([[-0.37731737,  0.5268799 ,  0.08515391, ...,  0.15354015,\n",
       "          0.51396936, -0.50008285],\n",
       "        [-0.4565794 ,  0.638372  ,  0.8758338 , ..., -1.1557547 ,\n",
       "         -0.37292305, -0.43407542],\n",
       "        [ 0.14142747, -0.08783931, -0.3186864 , ..., -0.26197883,\n",
       "          0.6963455 , -0.28463653],\n",
       "        ...,\n",
       "        [ 0.15152258,  0.29710206, -0.09133473, ..., -0.09607118,\n",
       "          0.95375955, -0.12217161],\n",
       "        [-0.05305808,  0.33465752, -0.52585274, ...,  0.07690534,\n",
       "          0.51694345, -0.49271977],\n",
       "        [-0.25598896,  0.22689773,  0.5134318 , ..., -0.3154517 ,\n",
       "          0.4354501 , -0.56531864]], dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_bert import extract_embeddings,POOL_NSP, POOL_MAX\n",
    "\n",
    "model_path = 'chinese_L-12_H-768_A-12'\n",
    "texts = ['今天天气真好', '我想出去钓鱼','但是还没有渔具怎么办','今天是个好日子']\n",
    "# texts=['语言模型']\n",
    "# texts=['我喜欢阅读','我不喜欢看书','机器人大赛没有人参加']\n",
    "# texts=['备注:一体化/自行运输 /先出后报/DHL快件 分送单号:CFT-9A001C']\n",
    "\n",
    "embeddings = extract_embeddings(model_path, texts,output_layer_num=1)\n",
    "embeddings\n",
    "\n",
    "\n",
    "###与通过加装模型然后进行predict结果是一样的，取cls对应的向量作为整个位置的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1 0.92131263\n",
      "d2 0.73755056\n"
     ]
    }
   ],
   "source": [
    "x=embeddings[0][0]\n",
    "y=embeddings[1][0]\n",
    "d1=np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "print('d1',d1)\n",
    "x=embeddings[0][0]\n",
    "y=embeddings[2][0]\n",
    "d2=np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "print('d2',d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each in embeddings:\n",
    "#     print(each[0].shape)#(768*layers),将每层的进行拼接\n",
    "# ##所以embeddings维度就是：(batch_size,seq_length,hidden_size)\n",
    "# ####句子长度没有被截断：还加上两个特殊字符cls sep,所以总长度加2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['[CLS]', '今', '天', '天', '气', '真', '好', '[SEP]', '我', '想', '出', '去', '钓', '鱼', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import extract_embeddings,POOL_NSP, POOL_MAX\n",
    "# Tokenization\n",
    "from keras_bert import Tokenizer\n",
    "\n",
    "model_path = 'chinese_L-12_H-768_A-12'\n",
    "# texts = ['今天天气真好', '我想出去钓鱼','但是还没有渔具怎么办','今天是个好日子']\n",
    "# texts=['我喜欢阅读','他特别爱看书']\n",
    "texts = [('今天天气真好', '我想出去钓鱼'),('但是还没有渔具怎么办','今天是个好日子')]\n",
    "\n",
    "tokens = tokenizer.tokenize(texts[0][0],texts[0][1])\n",
    "print('tokens',tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.41787237,  0.10842425, -0.10618674, ..., -0.6376673 ,\n",
       "          0.3150339 , -0.3306217 ],\n",
       "        [-0.25141335,  0.17533468,  0.8719655 , ..., -1.103228  ,\n",
       "         -0.31572154, -0.31728402],\n",
       "        [ 0.24454619, -0.20505098, -0.25032094, ...,  0.13061534,\n",
       "          0.9959889 , -0.24131359],\n",
       "        ...,\n",
       "        [ 1.040853  ,  0.36903498, -1.1642027 , ...,  0.66517913,\n",
       "          1.556844  , -1.3390598 ],\n",
       "        [ 0.87521255,  0.74877363, -1.0801226 , ...,  0.5517029 ,\n",
       "          1.0371956 , -0.39985517],\n",
       "        [ 0.49843264,  0.3043327 ,  0.19980821, ..., -0.2850037 ,\n",
       "          0.5264528 , -0.67115545]], dtype=float32),\n",
       " array([[-0.22058687,  1.0693215 ,  0.08066734, ...,  0.1018827 ,\n",
       "          0.48544875, -0.3140596 ],\n",
       "        [ 0.09858294,  0.4706137 ,  1.0824835 , ..., -0.82252586,\n",
       "         -0.48373666, -0.07129987],\n",
       "        [ 0.40670437,  0.18414694, -0.12365622, ...,  0.16479585,\n",
       "          0.39385623, -0.08197625],\n",
       "        ...,\n",
       "        [-0.17819348,  0.3309142 , -0.41032454, ...,  0.42099223,\n",
       "          1.1604339 , -0.2184361 ],\n",
       "        [-0.05786982,  0.09754853, -0.98505974, ...,  0.33313808,\n",
       "          0.45735335, -0.43439108],\n",
       "        [ 0.2876299 ,  0.61548376,  0.47607026, ...,  0.14142606,\n",
       "          0.74474543, -0.63409865]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings2 = extract_embeddings(model_path, texts,output_layer_num=2,poolings=[POOL_NSP, POOL_MAX])\n",
    "embeddings2 = extract_embeddings(model_path, texts,output_layer_num=1)\n",
    "\n",
    "embeddings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(embeddings2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 768)\n",
      "(20, 768)\n"
     ]
    }
   ],
   "source": [
    "for each in embeddings2:\n",
    "    print(each.shape)\n",
    "##embedding2维度是：(batchsize,(seq_length,hidden_size))\n",
    "##seq_length包含特殊字符：cls 、sep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.23948173,  0.59076923,  0.30380645, ...,  1.0177617 ,\n",
       "         1.556844  , -0.01932518], dtype=float32),\n",
       " array([-0.5101332 ,  1.3814349 ,  0.38318452, ...,  0.95225716,\n",
       "         1.3710293 , -0.03313443], dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras_bert import extract_embeddings, POOL_NSP, POOL_MAX\n",
    "\n",
    "model_path = 'xxx/yyy/uncased_L-12_H-768_A-12'\n",
    "texts = [\n",
    "    ('all work and no play', 'makes jack a dull boy'),\n",
    "    ('makes jack a dull boy', 'all work and no play'),\n",
    "]\n",
    "\n",
    "embeddings = extract_embeddings(model_path, texts, output_layer_num=4, poolings=[POOL_NSP, POOL_MAX])\n",
    "'''\n",
    "from keras_bert import extract_embeddings,POOL_NSP, POOL_MAX\n",
    "\n",
    "model_path = 'chinese_L-12_H-768_A-12'\n",
    "texts = [('今天天气真好', '我想出去钓鱼'),('但是还没有渔具怎么办','今天是个好日子')]\n",
    "\n",
    "embeddings3 = extract_embeddings(model_path, texts,output_layer_num=4,poolings=[POOL_NSP, POOL_MAX])\n",
    "embeddings3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6144,)\n",
      "(6144,)\n"
     ]
    }
   ],
   "source": [
    "for each in embeddings3:\n",
    "    print(each.shape)\n",
    "##6144=(2*768*4)POOL_NSP, POOL_MAX，都是768进行拼接，然后在把每层的拼接，最终得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['[CLS]', '运', '抵', '国', '(', '地', '区', '）', '智', '利', '[SEP]']\n",
      "[101, 6817, 2850, 1744, 113, 1765, 1277, 8021, 3255, 1164]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] [0.22144867479801178, 0.637603223323822, -0.515081524848938, -0.33984601497650146, 0.37335869669914246]\n",
      "运 [-0.7819637656211853, 0.6207656860351562, 0.5108278393745422, -0.23741640150547028, 0.48000359535217285]\n",
      "抵 [-0.28173917531967163, 0.06272527575492859, -0.8410854935646057, -0.5817319750785828, 0.09560249745845795]\n",
      "国 [0.415590763092041, -0.0028641410171985626, -0.5692856907844543, -0.46874672174453735, -0.032260019332170486]\n",
      "( [1.025439977645874, 0.5317772626876831, 0.8766087889671326, -0.482724130153656, 0.2723313570022583]\n",
      "地 [0.3468983769416809, 0.05775012448430061, 0.3927401006221771, -0.9270386099815369, 0.06529355049133301]\n",
      "区 [1.112783670425415, -0.07003981620073318, 0.07672543078660965, -0.5605531334877014, 0.5585036277770996]\n",
      "） [0.03459399193525314, 0.053688567131757736, -0.7230940461158752, -1.0295549631118774, 0.17501723766326904]\n",
      "智 [0.08102130889892578, 0.7572324275970459, -1.2627466917037964, 0.69561767578125, 0.35933664441108704]\n",
      "利 [-0.1220356747508049, -0.8534998297691345, -1.8006885051727295, 0.21395494043827057, 0.5996054410934448]\n",
      "[SEP] [0.20774222910404205, -0.20112498104572296, 0.5431217551231384, -0.07858557999134064, 0.03341935947537422]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import tensorflow as tf\n",
    "#关闭eager模式\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# 设置预训练模型的路径\n",
    "pretrained_path = 'chinese_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "# 构建字典\n",
    "# 也可以用 keras_bert 中的 load_vocabulary() 函数\n",
    "# 传入 vocab_path 即可\n",
    "# from keras_bert import load_vocabulary\n",
    "# token_dict = load_vocabulary(vocab_path)\n",
    "import codecs\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "# 加载预训练模型\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
    "\n",
    "# Tokenization\n",
    "from keras_bert import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "# text = '语言模型 chinese is great'\n",
    "# text='商品名称及规格型号'\n",
    "# text='境外收货人\\nDERCOCHILEREPUESTOSS.A.'\n",
    "# text='合同协议号\\n2019CICSA473-A'\n",
    "text='运抵国(地区）\\n智利'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# ['[CLS]', '语', '言', '模', '型', '[SEP]']\n",
    "print('tokens',tokens)\n",
    "indices, segments = tokenizer.encode(first=text, max_len=512)\n",
    "print(indices[:10])\n",
    "# [101, 6427, 6241, 3563, 1798, 102, 0, 0, 0, 0]\n",
    "print(segments[:10])\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 提取特征\n",
    "import numpy as np\n",
    "\n",
    "predicts = model.predict([np.array([indices]), np.array([segments])])[0]\n",
    "for i, token in enumerate(tokens):\n",
    "    print(token, predicts[i].tolist()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6144/4/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8604857435872377"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.random.random(10)\n",
    "y=np.random.random(10)\n",
    "\n",
    "#方法一：根据公式求解\n",
    "d1=np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=embeddings2[0][0]\n",
    "y=embeddings2[1][0]\n",
    "d1=np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'un', '##aff', '##able', '[SEP]']\n",
      "[0, 2, 3, 4, 1]\n",
      "[0, 0, 0, 0, 0]\n",
      "['[CLS]', 'un', '##aff', '##able', '[SEP]', '钢', '[SEP]']\n",
      "[0, 2, 3, 4, 1, 5, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import Tokenizer\n",
    "\n",
    "token_dict = {\n",
    "    '[CLS]': 0,\n",
    "    '[SEP]': 1,\n",
    "    'un': 2,\n",
    "    '##aff': 3,\n",
    "    '##able': 4,\n",
    "    '[UNK]': 5,\n",
    "}\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "print(tokenizer.tokenize('unaffable'))  # The result should be `['[CLS]', 'un', '##aff', '##able', '[SEP]']`\n",
    "indices, segments = tokenizer.encode('unaffable')\n",
    "print(indices)  # Should be `[0, 2, 3, 4, 1]`\n",
    "print(segments)  # Should be `[0, 0, 0, 0, 0]`\n",
    "\n",
    "print(tokenizer.tokenize(first='unaffable', second='钢'))\n",
    "# The result should be `['[CLS]', 'un', '##aff', '##able', '[SEP]', '钢', '[SEP]']`\n",
    "indices, segments = tokenizer.encode(first='unaffable', second='钢', max_len=10)\n",
    "print(indices)  # Should be `[0, 2, 3, 4, 1, 5, 1, 0, 0, 0]`\n",
    "print(segments)  # Should be `[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "tokenizer.tokenize(u'今天天气不错')\n",
    "# 输出是 ['[CLS]', u'今', u'天', u'天', u'气', u'不', u'错', '[SEP]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'un', '##aff', '##able', '[SEP]']\n",
      "[0, 2, 3, 4, 1]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import Tokenizer\n",
    "#字典\n",
    "token_dict = {\n",
    "    '[CLS]': 0,\n",
    "    '[SEP]': 1,\n",
    "    'un': 2,\n",
    "    '##aff': 3,\n",
    "    '##able': 4,\n",
    "    '[UNK]': 5,\n",
    "}\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "\n",
    "# 拆分单词实例\n",
    "print(tokenizer.tokenize('unaffable')) \n",
    "# ['[CLS]', 'un', '##aff', '##able', '[SEP]']\n",
    "\n",
    "# indices是字对应索引\n",
    "# segments表示索引对应位置上的字属于第一句话还是第二句话\n",
    "# 这里只有一句话 unaffable，所以segments都是0\n",
    "indices, segments = tokenizer.encode('unaffable')\n",
    "print(indices)  \n",
    "# [0, 2, 3, 4, 1]\n",
    "print(segments)  \n",
    "# [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们用同样的字典，拆分不存在 字典 中的单词，结果如下，可以看到英语中会直接把不存在字典中的部分直接按字母拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'un', '##k', '##n', '##o', '##w', '##n', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('unknown')) \n",
    "# ['[CLS]', 'un', '##k', '##n', '##o', '##w', '##n', '[SEP]']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 5, 5, 5, 5, 5, 1] [0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "indices, segments = tokenizer.encode('unknown')\n",
    "# [0, 2, 5, 5, 5, 5, 5, 1]\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(indices, segments )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是输入两句话的例子，encode 函数中 我们可以带上参数 max_len，只看文本拆分出来的 max_len 个字\n",
    "\n",
    "# 如果拆分完的字不超过max_len，则用 0 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'un', '##aff', '##able', '[SEP]', '钢', '[SEP]']\n",
      "[0, 2, 3, 4, 1, 5, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(first='unaffable', second='钢'))\n",
    "# ['[CLS]', 'un', '##aff', '##able', '[SEP]', '钢', '[SEP]']\n",
    "indices, segments = tokenizer.encode(first='unaffable', second='钢', max_len=10)\n",
    "print(indices)  \n",
    "# [0, 2, 3, 4, 1, 5, 1, 0, 0, 0]\n",
    "print(segments)  \n",
    "# [0, 0, 0, 0, 0, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 1], [0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意这个 max_len 包括 BERT 中的特殊 token，比如下面的代码\n",
    "\n",
    "tokenizer.encode('unaffable', max_len=3)\n",
    "# [0, 2, 1]\n",
    "# 我们得到的结果是 [0, 2, 1]，0 和 1 分别代表 [CLS] 和 [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这个例子里面，我们的不用 Tokenizer 将文本拆分成 “字”，而是使用 “词” 级别作为模型的输入\n",
    "\n",
    "这里跟 keras 的文本处理很像，可以参考下面这篇文章\n",
    "\n",
    "https://www.cnblogs.com/dogecheng/p/11565530.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 20, 25), (28 700         Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 20, 25)       50          Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 20, 25)       0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 20, 25)       500         Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 20, 25)       0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 20, 25)       50          Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 20, 25)       2600        Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 20, 25)       0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 20, 25)       2600        Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 20, 25)       0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 20, 25)       50          Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 20, 25)       5125        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 20, 25)       0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 20, 25)       0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 20, 25)       50          Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 20, 25)       2600        Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 20, 25)       0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 20, 25)       0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 20, 25)       50          Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 20, 25)       5125        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 20, 25)       0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 20, 25)       0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 20, 25)       50          Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 20, 25)       2600        Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 20, 25)       0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 20, 25)       0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 20, 25)       50          Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 20, 25)       5125        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 20, 25)       0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 20, 25)       0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 20, 25)       50          Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 20, 25)       2600        Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 20, 25)       0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 20, 25)       0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 20, 25)       50          Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 20, 25)       5125        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 20, 25)       0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 20, 25)       0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 20, 25)       50          Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, 20, 25)       650         Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, 20, 25)       50          MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 25)           0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Sim (EmbeddingSimilarity)   (None, 20, 28)       28          MLM-Norm[0][0]                   \n",
      "                                                                 Embedding-Token[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Masked (InputLayer)       (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 25)           650         Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "MLM (Masked)                    (None, 20, 28)       0           MLM-Sim[0][0]                    \n",
      "                                                                 Input-Masked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "NSP (Dense)                     (None, 2)            52          NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 96,630\n",
      "Trainable params: 96,630\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 55s 55ms/step - loss: 1.2136 - MLM_loss: 0.4413 - NSP_loss: 0.7724 - val_loss: 0.7275 - val_MLM_loss: 0.4560 - val_NSP_loss: 0.5661\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.9442 - MLM_loss: 0.4423 - NSP_loss: 0.5020 - val_loss: 0.9931 - val_MLM_loss: 0.4404 - val_NSP_loss: 0.3157\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.7619 - MLM_loss: 0.4353 - NSP_loss: 0.3266 - val_loss: 0.4940 - val_MLM_loss: 0.4329 - val_NSP_loss: 0.2038\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5861 - MLM_loss: 0.4274 - NSP_loss: 0.1587 - val_loss: 0.2761 - val_MLM_loss: 0.4337 - val_NSP_loss: 0.0474\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.5066 - MLM_loss: 0.4272 - NSP_loss: 0.0794 - val_loss: 0.3915 - val_MLM_loss: 0.4265 - val_NSP_loss: 0.0143\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs\n",
    "\n",
    "\n",
    "# 输入示例\n",
    "sentence_pairs = [\n",
    "    [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n",
    "    [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n",
    "    [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n",
    "]\n",
    "\n",
    "# 构建 token 字典\n",
    "# 这个字典存放的是【词】\n",
    "token_dict = get_base_dict()  \n",
    "# get_base_dict()返回一个字典\n",
    "# 字典预置了一些特殊token，具体内容如下\n",
    "# {'': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
    "for pairs in sentence_pairs:\n",
    "    for token in pairs[0] + pairs[1]:\n",
    "        if token not in token_dict:\n",
    "            token_dict[token] = len(token_dict)\n",
    "# token_dict 是由词组成的字典，大致如下\n",
    "# {'': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'all': 5, 'work': 6,..., 'me': 26, 'more': 27}\n",
    "\n",
    "token_list = list(token_dict.keys())\n",
    "\n",
    "\n",
    "# 构建和训练模型\n",
    "model = get_model(\n",
    "    token_num=len(token_dict),\n",
    "    head_num=5,\n",
    "    transformer_num=12,\n",
    "    embed_dim=25,\n",
    "    feed_forward_dim=100,\n",
    "    seq_len=20,\n",
    "    pos_num=20,\n",
    "    dropout_rate=0.05,\n",
    ")\n",
    "compile_model(model)\n",
    "model.summary()\n",
    "\n",
    "def _generator():\n",
    "    while True:\n",
    "        yield gen_batch_inputs(\n",
    "            sentence_pairs,\n",
    "            token_dict,\n",
    "            token_list,\n",
    "            seq_len=20,\n",
    "            mask_rate=0.3,\n",
    "            swap_sentence_rate=1.0,\n",
    "        )\n",
    "\n",
    "model.fit_generator(\n",
    "# 这里测试集和验证集使用了同样的数据\n",
    "# 实际中使用时不能这样\n",
    "    generator=_generator(),\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=5,\n",
    "    validation_data=_generator(),\n",
    "    validation_steps=100,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# 使用训练好的模型\n",
    "# 取出 输入层 和 最后一个特征提取层\n",
    "inputs, output_layer = get_model(\n",
    "    token_num=len(token_dict),\n",
    "    head_num=5,\n",
    "    transformer_num=12,\n",
    "    embed_dim=25,\n",
    "    feed_forward_dim=100,\n",
    "    seq_len=20,\n",
    "    pos_num=20,\n",
    "    dropout_rate=0.05,\n",
    "    training=False,\n",
    "    trainable=False,\n",
    "    output_layer_num=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor 'Input-Token_5:0' shape=(None, 20) dtype=float32>,\n",
       "  <tf.Tensor 'Input-Segment_5:0' shape=(None, 20) dtype=float32>],\n",
       " <tf.Tensor 'Encoder-Output/concat:0' shape=(None, 20, 100) dtype=float32>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以使用 load_trained_model_from_checkpoint() 函数使用本地已经下载好的预训练模型，可以从 BERT 的 github 上获取下载地址\n",
    "\n",
    "# 谷歌BERT地址：https://github.com/google-research/bert\n",
    "\n",
    "# 中文预训练BERT-wwm：https://github.com/ymcui/Chinese-BERT-wwm\n",
    "\n",
    "# 下面是使用预训练模型提取输入文本的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6427, 6241, 3563, 1798, 102, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] [-0.632510244846344, 0.20302419364452362, 0.07936591655015945, -0.03284311294555664, 0.5668085813522339]\n",
      "语 [-0.7588356733322144, 0.09651871025562286, 1.0718743801116943, 0.0050378888845443726, 0.688799262046814]\n",
      "言 [0.5477027893066406, -0.7921169400215149, 0.44435206055641174, -0.7112652659416199, 1.204888939857483]\n",
      "模 [-0.2924236059188843, 0.6052713990211487, 0.4996863305568695, -0.424579381942749, 0.42855390906333923]\n",
      "型 [-0.7473453879356384, 0.4943157732486725, 0.7185168862342834, -0.8723524212837219, 0.8349595665931702]\n",
      "[SEP] [-0.8741373419761658, -0.21650327742099762, 1.338838815689087, -0.10587075352668762, 0.39608973264694214]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置预训练模型的路径\n",
    "pretrained_path = 'chinese_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "# 构建字典\n",
    "# 也可以用 keras_bert 中的 load_vocabulary() 函数\n",
    "# 传入 vocab_path 即可\n",
    "# from keras_bert import load_vocabulary\n",
    "# token_dict = load_vocabulary(vocab_path)\n",
    "import codecs\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "# 加载预训练模型\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
    "\n",
    "# Tokenization\n",
    "from keras_bert import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "text = '语言模型'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# ['[CLS]', '语', '言', '模', '型', '[SEP]']\n",
    "indices, segments = tokenizer.encode(first=text, max_len=512)\n",
    "print(indices[:10])\n",
    "# [101, 6427, 6241, 3563, 1798, 102, 0, 0, 0, 0]\n",
    "print(segments[:10])\n",
    "# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 提取特征\n",
    "import numpy as np\n",
    "\n",
    "predicts = model.predict([np.array([indices]), np.array([segments])])[0]\n",
    "for i, token in enumerate(tokens):\n",
    "    print(token, predicts[i].tolist()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面我们用预训练模型预测句子中被 MASKED 掉的词语是什么\n",
    "###加载并使用预训练的模型进行预测，而不是用于后续的微调什么的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill with:  ['数', '学']\n"
     ]
    }
   ],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "token_dict_rev = {v: k for k, v in token_dict.items()}\n",
    "\n",
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
    "\n",
    "text = '数学是利用符号语言研究数量、结构、变化以及空间等概念的一门学科'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens[1] = tokens[2] = '[MASK]'# ['[CLS]', '[MASK]', '[MASK]', '是', '利',..., '学', '科', '[SEP]']\n",
    "\n",
    "indices = np.array([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens))])\n",
    "segments = np.array([[0] * len(tokens) + [0] * (512 - len(tokens))])\n",
    "masks = np.array([[0, 1, 1] + [0] * (512 - 3)])\n",
    "predicts = model.predict([indices, segments, masks])[0].argmax(axis=-1).tolist()\n",
    "print('Fill with: ', list(map(lambda x: token_dict_rev[x], predicts[0][1:3])))\n",
    "# Fill with:  ['数', '学']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
